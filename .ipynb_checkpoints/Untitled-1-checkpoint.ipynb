{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import re\n",
    "import string\n",
    "import pickle\n",
    "import requests\n",
    "import json\n",
    "from indoNLP.preprocessing import replace_word_elongation, emoji_to_words, replace_slang\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scraperKey(keyword,date_since,date_until):\n",
    "    query = keyword+\" lang:id until:\"+str(date_until)+\" since:\"+str(date_since)\n",
    "    now = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    # requests.post('{}/progress/twitter/new'.format(APIurl),{'dateGet':now,'keyword':keyword,'dateSince':date_since,'dateUntil':date_until,'status':1,'source':'tw'} )\n",
    "    #print(query)\n",
    "    #print(datetime.now())\n",
    "    #print(\"Sedang Mengumpulkan Data Twitter...\")\n",
    "    tweets = []\n",
    "    while len(tweets) <= 20:\n",
    "        for tweet in sntwitter.TwitterSearchScraper(query).get_items():\n",
    "            tweets.append([datetime.now().date(), keyword, tweet.date, tweet.user.username, tweet.content, tweet.hashtags,tweet.mentionedUsers, tweet.likeCount, tweet.retweetCount, tweet.replyCount])\n",
    "        \n",
    "        \n",
    "        \n",
    "    df = pd.DataFrame(tweets, columns=['timestamp','keyword','date', 'user', 'tweet', 'hashtags', 'mentions', 'likeCount', 'retweetCount', 'replyCount'])\n",
    "    return df,now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ch/fxrhbppn0ds9grnhwp1j2qfm0000gn/T/ipykernel_11391/2855697321.py:11: FutureWarning: content is deprecated, use rawContent instead\n",
      "  tweets.append([datetime.now().date(), keyword, tweet.date, tweet.user.username, tweet.content, tweet.hashtags,tweet.mentionedUsers, tweet.likeCount, tweet.retweetCount, tweet.replyCount])\n"
     ]
    }
   ],
   "source": [
    "df,now = scraperKey(\"jogja\",'2023-02-20','2023-02-24')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df[['tweet']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaningText(text):\n",
    "    text = re.sub(r'@[A-Za-z0-9]+', '', text) # remove mentions\n",
    "    text = re.sub(r'#[A-Za-z0-9]+', '', text) # remove hashtag\n",
    "    text = re.sub(r'RT[\\s]', '', text) # remove RT\n",
    "    text = re.sub(r\"http\\S+\", '', text) # remove link\n",
    "    text = re.sub(r'[0-9]+', '', text) # remove numbers\n",
    "    # text = text.encode(\"ascii\", \"ignore\").decode() #remove emojis\n",
    "\n",
    "    text = text.replace('â€¢', '') # replace new line into space\n",
    "    text = text.replace('\\n', ' ') # replace new line into space\n",
    "    # text = [pipe(word) for word in text]\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation)) # remove all punctuations\n",
    "    text = text.strip(' ') # remove characters space from both left and right text\n",
    "\n",
    "    return text\n",
    "\n",
    "def casefoldingText(text): # Converting all the characters in a text into lower case\n",
    "    text = text.lower() \n",
    "    return text\n",
    "\n",
    "def tokenizingText(text): # Tokenizing or splitting a string, text into a list of tokens\n",
    "    text = word_tokenize(text) \n",
    "    return text\n",
    "\n",
    "def filteringText(text): # Remove stopwors in a text\n",
    "    listStopwords = set(stopwords.words('indonesian'))\n",
    "    text = [w for w in text if not w in listStopwords]\n",
    "    return text\n",
    "\n",
    "def stemmingText(text): # Reducing a word to its word stem that affixes to suffixes and prefixes or to the roots of words\n",
    "    #factory = StemmerFactory()\n",
    "    from nltk.stem import PorterStemmer\n",
    "    stemmer = PorterStemmer()\n",
    "    text = [stemmer.stem(word) for word in text]\n",
    "    return text\n",
    "\n",
    "def replaceElongation(text): # Reducing a word to its word stem that affixes to suffixes and prefixes or to the roots of words\n",
    "    text = [replace_word_elongation(word) for word in text]\n",
    "    \n",
    "    return text\n",
    "\n",
    "def replaceEmojis(text): # Reducing a word to its word stem that affixes to suffixes and prefixes or to the roots of words\n",
    "    text = ' '.join([emoji_to_words(word, lang='en').replace('!',' ').replace('_','') for word in text.split(' ')])\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    text = text.encode(\"ascii\", \"ignore\").decode()\n",
    "    return text\n",
    "\n",
    "def toSentence(list_words): # Convert list of words into sentence\n",
    "    sentence = ' '.join(word for word in list_words)\n",
    "    return sentence\n",
    "\n",
    "alay_dict = pd.read_csv('colloquial-indonesian-lexicon.csv', encoding='latin-1', header=None)\n",
    "alay_dict = alay_dict.rename(columns={0: 'original', 1: 'replacement'})\n",
    "alay_dict_map = dict(zip(alay_dict['original'], alay_dict['replacement']))\n",
    "\n",
    "def normalize_alay(text):\n",
    " text = ' '.join([alay_dict_map[word] if word in alay_dict_map else word for word in text.split(' ')])\n",
    " text = re.sub(' +', ' ', text)\n",
    " return text\n",
    "\n",
    "def replaceSlang(text):\n",
    "    text = [replace_slang(word) for word in text]\n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# pipe = pipeline([replace_word_elongation, replace_slang])\n",
    "# pipe(\"MAx  Kota Besar Kota Kecil ga jaminan sekarang Kak klo perhatian dan didikan keluarga Kurang ini Aku dom malah di Ibukota Jabar Kurang Besar gimana lg coba\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_to_words('ðŸ¤¬',lang='en').replace('!',' ').replace('_','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text_clean'] = data['tweet'].apply(cleaningText)\n",
    "data['text_clean'] = data['text_clean'].apply(casefoldingText)\n",
    "data['text_clean'] = data['text_clean'].apply(replaceEmojis)\n",
    "data['text_clean'] = data['text_clean'].apply(normalize_alay)\n",
    "data['text_preprocessed'] = data['text_clean'].apply(tokenizingText)\n",
    "data['text_preprocessed'] = data['text_preprocessed'].apply(replaceElongation)\n",
    "data['text_preprocessed'] = data['text_preprocessed'].apply(replaceSlang)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pickle.load(open('tfidf_svc.pickle','rb'))\n",
    "tfidf_vectorizer =  pickle.load(open('tfidf_vectorizer.pickle','rb'))\n",
    "\n",
    "X = data['text_preprocessed'].apply(toSentence)\n",
    "X = tfidf_vectorizer.transform(X.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X)\n",
    "data['sentiment'] = y_pred\n",
    "\n",
    "polarity_decode = {0 : 'Negative', 1 : 'Neutral', 2 : 'Positive'}\n",
    "data['sentiment'] = data['sentiment'].map(polarity_decode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proba = model.predict_proba(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proba[194]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = []\n",
    "for i in proba:\n",
    "    ilist = i.tolist()\n",
    "    max_prob = 1\n",
    "    if i[2] <= 0.80 and i[0] <= 0.80:\n",
    "        max_prob = 1\n",
    "    else:\n",
    "        max_prob = ilist.index(max(ilist))\n",
    "    \n",
    "    \n",
    "    probs.append(max_prob)\n",
    "\n",
    "print(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probs = []\n",
    "# for i in proba:\n",
    "#     ilist = i.tolist()\n",
    "#     max_prob = 1\n",
    "#     if i[2] == max(i):\n",
    "#         max_prob = 2\n",
    "#     elif i[0] <= 0.80:\n",
    "#         max_prob = 1\n",
    "#     else:\n",
    "#         max_prob = ilist.index(max(ilist))\n",
    "    \n",
    "    \n",
    "#     probs.append(max_prob)\n",
    "\n",
    "# print(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jml = len(proba)\n",
    "count = 0\n",
    "for i in proba:\n",
    "    if max(i) <= 0.50:\n",
    "        count += 1\n",
    "\n",
    "print(jml)\n",
    "print(\"Jumlah dibawah batas : \"+ str(count))\n",
    "print(\"Persentase dibawah batas : \"+str(((count/jml)*100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probs = []\n",
    "# for i in proba:\n",
    "#     max_prob = 1\n",
    "#     if i[0] == max(i):\n",
    "#         max_prob = 0\n",
    "#     elif i[2] == max(i):\n",
    "#         max_prob = 2\n",
    "#     else:\n",
    "#         max_prob = 1\n",
    "\n",
    "#     if max(i) < 0.75:\n",
    "#         max_prob = 1\n",
    "#     probs.append(max_prob)\n",
    "\n",
    "# print(probs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['alt'] = probs\n",
    "data['alt'] = data['alt'].map(polarity_decode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['alt'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"data3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "872e9ac453f67b7e35588ebe4d602923bc34b4c5daddde36c325676c5fe52d29"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
