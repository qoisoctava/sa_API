{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import re\n",
    "import string\n",
    "import pickle\n",
    "import requests\n",
    "import json\n",
    "from indoNLP.preprocessing import replace_word_elongation, emoji_to_words, replace_slang\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build\n",
    "\n",
    "# api key youtube\n",
    "api_key = 'AIzaSyAOALHxbc03AvhCtaU0vYBaW6hAj8XTWYc'\n",
    "\n",
    "def video_comments(video_id):\n",
    "    # empty list for storing reply\n",
    "\treplies = []\n",
    "\n",
    "\t# creating youtube resource object\n",
    "\tyoutube = build('youtube', 'v3', developerKey=api_key)\n",
    "\t\n",
    "\tvideo = youtube.videos().list(part='snippet', id=video_id).execute()\n",
    "\n",
    "\ttitle = ' '.join([item['snippet']['title'] for item in video['items']])\n",
    "\tvideoDate = ' '.join([item['snippet']['publishedAt'] for item in video['items']])\n",
    "\tchannel = ' '.join([item['snippet']['channelTitle'] for item in video['items']])\n",
    "    \n",
    "\t# retrieve youtube video results\n",
    "\tvideo_response = youtube.commentThreads().list(part='snippet,replies', videoId=video_id).execute()\n",
    "\n",
    "    \n",
    "\t# iterate video response\n",
    "\twhile video_response:\n",
    "\t\t\n",
    "\t\t# extracting required info\n",
    "\t\t# from each result object\n",
    "\t\tfor item in video_response['items']:\n",
    "\t\t\t\n",
    "\t\t\t# Extracting comments ()\n",
    "\t\t\tpublished = item['snippet']['topLevelComment']['snippet']['publishedAt']\n",
    "\t\t\tuser = item['snippet']['topLevelComment']['snippet']['authorDisplayName']\n",
    "\n",
    "\t\t\t# Extracting comments\n",
    "\t\t\tcomment = item['snippet']['topLevelComment']['snippet']['textDisplay']\n",
    "\t\t\tlikeCount = item['snippet']['topLevelComment']['snippet']['likeCount']\n",
    "\n",
    "\t\t\treplies.append([videoDate,channel,title,published, user, comment, likeCount])\n",
    "\t\t\t\n",
    "\t\t\t# counting number of reply of comment\n",
    "\t\t\treplycount = item['snippet']['totalReplyCount']\n",
    "\n",
    "\t\t\t# if reply is there\n",
    "\t\t\tif replycount>0:\n",
    "\t\t\t\t# iterate through all reply\n",
    "\t\t\t\tfor reply in item['replies']['comments']:\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\t# Extract reply\n",
    "\t\t\t\t\tpublished = reply['snippet']['publishedAt']\n",
    "\t\t\t\t\tuser = reply['snippet']['authorDisplayName']\n",
    "\t\t\t\t\trepl = reply['snippet']['textDisplay']\n",
    "\t\t\t\t\tlikeCount = reply['snippet']['likeCount']\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\t# Store reply is list\n",
    "\t\t\t\t\t#replies.append(reply)\n",
    "\t\t\t\t\treplies.append([videoDate,channel,title,published, user, repl, likeCount])\n",
    "\n",
    "\t\t\t# print comment with list of reply\n",
    "\t\t\t#print(comment, replies, end = '\\n\\n')\n",
    "\n",
    "\t\t\t# empty reply list\n",
    "\t\t\t#replies = []\n",
    "\n",
    "\t\t# Again repeat\n",
    "\t\tif 'nextPageToken' in video_response:\n",
    "\t\t\tvideo_response = youtube.commentThreads().list(\n",
    "\t\t\t\t\tpart = 'snippet,replies',\n",
    "\t\t\t\t\tpageToken = video_response['nextPageToken'], \n",
    "\t\t\t\t\tvideoId = video_id\n",
    "\t\t\t\t).execute()\n",
    "\t\telse:\n",
    "\t\t\tbreak\n",
    "\t#endwhile\n",
    "\treturn replies\n",
    "\n",
    "def youtube(video_id):\n",
    "    now = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "    # newProgress = {'get_date':now, 'channel_name':'loading...', 'title':'loading...', 'video_date':datetime.now(),  'status':1}\n",
    "    print(\"Sedang Mengumpulkan Data youtube...\")\n",
    "    comments = video_comments(video_id)\n",
    "    df = pd.DataFrame(comments, columns=['video_date','channel_name','title','comment_date', 'commentator', 'content', 'like_count'])\n",
    "    \n",
    "       \n",
    "    return df,now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def scraperKey(keyword,date_since,date_until):\n",
    "#     query = keyword+\" lang:id until:\"+str(date_until)+\" since:\"+str(date_since)\n",
    "#     now = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "#     # requests.post('{}/progress/twitter/new'.format(APIurl),{'dateGet':now,'keyword':keyword,'dateSince':date_since,'dateUntil':date_until,'status':1,'source':'tw'} )\n",
    "#     #print(query)\n",
    "#     #print(datetime.now())\n",
    "#     #print(\"Sedang Mengumpulkan Data Twitter...\")\n",
    "#     tweets = []\n",
    "#     while len(tweets) <= 20:\n",
    "#         for tweet in sntwitter.TwitterSearchScraper(query).get_items():\n",
    "#             tweets.append([datetime.now().date(), keyword, tweet.date, tweet.user.username, tweet.content, tweet.hashtags,tweet.mentionedUsers, tweet.likeCount, tweet.retweetCount, tweet.replyCount])\n",
    "        \n",
    "        \n",
    "        \n",
    "#     df = pd.DataFrame(tweets, columns=['timestamp','keyword','date', 'user', 'tweet', 'hashtags', 'mentions', 'likeCount', 'retweetCount', 'replyCount'])\n",
    "#     return df,now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df,now = scraperKey('pj heru','2023-04-18','2023-04-20')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sedang Mengumpulkan Data youtube...\n"
     ]
    }
   ],
   "source": [
    "df,now = youtube('VcI9HxyY2i0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df[['content']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaningText(text):\n",
    "    text = re.sub(r'@[A-Za-z0-9]+', '', text) # remove mentions\n",
    "    text = re.sub(r'#[A-Za-z0-9]+', '', text) # remove hashtag\n",
    "    text = re.sub(r'RT[\\s]', '', text) # remove RT\n",
    "    text = re.sub(r\"http\\S+\", '', text) # remove link\n",
    "    text = re.sub(r'[0-9]+', '', text) # remove numbers\n",
    "    text = re.sub(re.compile('<.*?>') , '', text) # remove numbers\n",
    "    # text = text.encode(\"ascii\", \"ignore\").decode() #remove emojis\n",
    "\n",
    "    text = text.replace('â€¢', ' ') # replace new line into space\n",
    "    text = text.replace('\\n', ' ') # replace new line into space\n",
    "    # text = [pipe(word) for word in text]\n",
    "    # text = text.translate(str.maketrans(' ', ' ', string.punctuation)) # remove all punctuations\n",
    "    text = text.translate(str.maketrans(string.punctuation, ' '*len(string.punctuation))) # remove all punctuations\n",
    "    text = text.strip(' ') # remove characters space from both left and right text\n",
    "\n",
    "    return text\n",
    "\n",
    "def casefoldingText(text): # Converting all the characters in a text into lower case\n",
    "    text = text.lower() \n",
    "    return text\n",
    "\n",
    "def tokenizingText(text): # Tokenizing or splitting a string, text into a list of tokens\n",
    "    text = word_tokenize(text) \n",
    "    return text\n",
    "\n",
    "def filteringText(text): # Remove stopwors in a text\n",
    "    listStopwords = set(stopwords.words('indonesian'))\n",
    "    text = [w for w in text if not w in listStopwords]\n",
    "    return text\n",
    "\n",
    "def stemmingText(text): # Reducing a word to its word stem that affixes to suffixes and prefixes or to the roots of words\n",
    "    #factory = StemmerFactory()\n",
    "    from nltk.stem import PorterStemmer\n",
    "    stemmer = PorterStemmer()\n",
    "    text = [stemmer.stem(word) for word in text]\n",
    "    return text\n",
    "\n",
    "def replaceElongation(text): # Reducing a word to its word stem that affixes to suffixes and prefixes or to the roots of words\n",
    "    text = [replace_word_elongation(word) for word in text]\n",
    "    \n",
    "    return text\n",
    "\n",
    "def replaceEmojis(text): # Reducing a word to its word stem that affixes to suffixes and prefixes or to the roots of words\n",
    "    text = ' '.join([emoji_to_words(word, lang='en').replace('!',' ').replace('_','') for word in text.split(' ')])\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    text = text.encode(\"ascii\", \"ignore\").decode()\n",
    "    return text\n",
    "\n",
    "def toSentence(list_words): # Convert list of words into sentence\n",
    "    sentence = ' '.join(word for word in list_words)\n",
    "    return sentence\n",
    "\n",
    "alay_dict = pd.read_csv('colloquial-indonesian-lexicon.csv', encoding='latin-1', header=None)\n",
    "alay_dict = alay_dict.rename(columns={0: 'original', 1: 'replacement'})\n",
    "alay_dict_map = dict(zip(alay_dict['original'], alay_dict['replacement']))\n",
    "\n",
    "def normalize_alay(text):\n",
    " text = ' '.join([alay_dict_map[word] if word in alay_dict_map else word for word in text.split(' ')])\n",
    " text = re.sub(' +', ' ', text)\n",
    " return text\n",
    "\n",
    "def replaceSlang(text):\n",
    "    text = [replace_slang(word) for word in text]\n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# pipe = pipeline([replace_word_elongation, replace_slang])\n",
    "# pipe(\"MAx  Kota Besar Kota Kecil ga jaminan sekarang Kak klo perhatian dan didikan keluarga Kurang ini Aku dom malah di Ibukota Jabar Kurang Besar gimana lg coba\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' facewithsymbolsonmouth '"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# emoji_to_words('ðŸ¤¬',lang='en').replace('!',' ').replace('_','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text_clean'] = data['content'].apply(cleaningText)\n",
    "data['casefolding'] = data['text_clean'].apply(casefoldingText)\n",
    "data['emojis'] = data['casefolding'].apply(replaceEmojis)\n",
    "data['alay'] = data['emojis'].apply(normalize_alay)\n",
    "data['tokenizing'] = data['alay'].apply(tokenizingText)\n",
    "data['elongation'] = data['tokenizing'].apply(replaceElongation)\n",
    "# data['text_preprocessed'] = data['text_preprocessed'].apply(replaceSlang)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pickle.load(open('tfidf_svc.pickle','rb'))\n",
    "tfidf_vectorizer =  pickle.load(open('tfidf_vectorizer.pickle','rb'))\n",
    "\n",
    "X = data['elongation'].apply(toSentence)\n",
    "X = tfidf_vectorizer.transform(X.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X)\n",
    "data['sentiment'] = y_pred\n",
    "\n",
    "polarity_decode = {0 : 'Negative', 1 : 'Neutral', 2 : 'Positive'}\n",
    "data['sentiment'] = data['sentiment'].map(polarity_decode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 1 1 1 2 0 0 1 1 2 0 2 0 1 0 0 2 2 2 0 0 2 0 0 1 1 0 0 1 1 0 1 2 0 0\n",
      " 1 1 2 0 0 0 1 1 2 0 1 1 0 0 0 0 1 1 0 2 0 0 2 0 2 0 0 0 1 0 0 1 0 1 0 0 0\n",
      " 0 0 0 1 0 0 0 1 2 0 0 1 0 0 0 0 1 1 0 0 2 2 1 0 1 1 0 0 0 0 2 1 0 0 0 1 0\n",
      " 2 2 2 0 2 1 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1 2 0 2 0 1 0 1 0 0 0 0 0 0 2 2 1\n",
      " 1]\n"
     ]
    }
   ],
   "source": [
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "proba = model.predict_proba(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proba[194]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "probs = []\n",
    "for i in proba:\n",
    "    ilist = i.tolist()\n",
    "    max_prob = 1\n",
    "    if i[2] <= 0.80 and i[0] <= 0.80:\n",
    "        max_prob = 1\n",
    "    else:\n",
    "        max_prob = ilist.index(max(ilist))\n",
    "    \n",
    "    \n",
    "    probs.append(max_prob)\n",
    "\n",
    "print(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probs = []\n",
    "# for i in proba:\n",
    "#     ilist = i.tolist()\n",
    "#     max_prob = 1\n",
    "#     if i[2] == max(i):\n",
    "#         max_prob = 2\n",
    "#     elif i[0] <= 0.80:\n",
    "#         max_prob = 1\n",
    "#     else:\n",
    "#         max_prob = ilist.index(max(ilist))\n",
    "    \n",
    "    \n",
    "#     probs.append(max_prob)\n",
    "\n",
    "# print(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "149\n",
      "Jumlah dibawah batas : 15\n",
      "Persentase dibawah batas : 10.06711409395973\n"
     ]
    }
   ],
   "source": [
    "jml = len(proba)\n",
    "count = 0\n",
    "for i in proba:\n",
    "    if max(i) <= 0.50:\n",
    "        count += 1\n",
    "\n",
    "print(jml)\n",
    "print(\"Jumlah dibawah batas : \"+ str(count))\n",
    "print(\"Persentase dibawah batas : \"+str(((count/jml)*100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probs = []\n",
    "# for i in proba:\n",
    "#     max_prob = 1\n",
    "#     if i[0] == max(i):\n",
    "#         max_prob = 0\n",
    "#     elif i[2] == max(i):\n",
    "#         max_prob = 2\n",
    "#     else:\n",
    "#         max_prob = 1\n",
    "\n",
    "#     if max(i) < 0.75:\n",
    "#         max_prob = 1\n",
    "#     probs.append(max_prob)\n",
    "\n",
    "# print(probs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['alt'] = probs\n",
    "data['alt'] = data['alt'].map(polarity_decode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Negative    81\n",
       "Neutral     43\n",
       "Positive    25\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Neutral     119\n",
       "Negative     25\n",
       "Positive      5\n",
       "Name: alt, dtype: int64"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['alt'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.to_csv(\"data_bab4-2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "872e9ac453f67b7e35588ebe4d602923bc34b4c5daddde36c325676c5fe52d29"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
