{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import re\n",
    "import string\n",
    "import pickle\n",
    "import requests\n",
    "import json\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ch/fxrhbppn0ds9grnhwp1j2qfm0000gn/T/ipykernel_47150/1179595794.py:1: DeprecationWarning: Please use `csr_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csr` namespace is deprecated.\n",
      "  model = pickle.load(open('./old/modelSVC.pickle','rb'))\n",
      "/Users/qoisoctava/miniforge3/envs/pySA/lib/python3.9/site-packages/sklearn/base.py:288: UserWarning: Trying to unpickle estimator SVC from version 1.1.2 when using version 1.2.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/var/folders/ch/fxrhbppn0ds9grnhwp1j2qfm0000gn/T/ipykernel_47150/1179595794.py:2: DeprecationWarning: Please use `csr_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csr` namespace is deprecated.\n",
      "  tfidf_vectorizer =  pickle.load(open('./old/tfidf_vectorizer.pickle','rb'))\n",
      "/Users/qoisoctava/miniforge3/envs/pySA/lib/python3.9/site-packages/sklearn/base.py:288: UserWarning: Trying to unpickle estimator TfidfTransformer from version 1.1.2 when using version 1.2.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/qoisoctava/miniforge3/envs/pySA/lib/python3.9/site-packages/sklearn/base.py:288: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 1.1.2 when using version 1.2.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = pickle.load(open('./old/modelSVC.pickle','rb'))\n",
    "tfidf_vectorizer =  pickle.load(open('./old/tfidf_vectorizer.pickle','rb'))\n",
    "APIurl = 'http://127.0.0.1:3000'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scraperKey(keyword,date_since,date_until):\n",
    "    query = keyword+\" lang:id until:\"+str(date_until)+\" since:\"+str(date_since)\n",
    "    now = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    # requests.post('{}/progress/twitter/new'.format(APIurl),{'dateGet':now,'keyword':keyword,'dateSince':date_since,'dateUntil':date_until,'status':1,'source':'tw'} )\n",
    "    #print(query)\n",
    "    #print(datetime.now())\n",
    "    #print(\"Sedang Mengumpulkan Data Twitter...\")\n",
    "    tweets = []\n",
    "    while len(tweets) <= 20:\n",
    "        for tweet in sntwitter.TwitterSearchScraper(query).get_items():\n",
    "            tweets.append([datetime.now().date(), keyword, tweet.date, tweet.user.username, tweet.content, tweet.hashtags,tweet.mentionedUsers, tweet.likeCount, tweet.retweetCount, tweet.replyCount])\n",
    "        \n",
    "        \n",
    "        \n",
    "    df = pd.DataFrame(tweets, columns=['timestamp','keyword','date', 'user', 'tweet', 'hashtags', 'mentions', 'likeCount', 'retweetCount', 'replyCount'])\n",
    "    return df,now\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from indoNLP.preprocessing import pipeline, replace_word_elongation, replace_slang, emoji_to_words\n",
    "pipe = pipeline([emoji_to_words, replace_word_elongation, replace_slang])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaningText(text):\n",
    "    text = re.sub(r'@[A-Za-z0-9]+', '', text) # remove mentions\n",
    "    text = re.sub(r'#[A-Za-z0-9]+', '', text) # remove hashtag\n",
    "    text = re.sub(r'RT[\\s]', '', text) # remove RT\n",
    "    text = re.sub(r\"http\\S+\", '', text) # remove link\n",
    "    # text = re.sub(r'[0-9]+', '', text) # remove numbers\n",
    "    # text = text.encode(\"ascii\", \"ignore\").decode() #remove emojis\n",
    "\n",
    "    text = text.replace('\\n', ' ') # replace new line into space\n",
    "    text = [pipe(word) for word in text]\n",
    "    # text = text.translate(str.maketrans('', '', string.punctuation)) # remove all punctuations\n",
    "    # text = text.strip(' ') # remove characters space from both left and right text\n",
    "\n",
    "    return text\n",
    "\n",
    "def casefoldingText(text): # Converting all the characters in a text into lower case\n",
    "    text = text.lower() \n",
    "    return text\n",
    "\n",
    "def tokenizingText(text): # Tokenizing or splitting a string, text into a list of tokens\n",
    "    text = word_tokenize(text) \n",
    "    return text\n",
    "\n",
    "def filteringText(text): # Remove stopwors in a text\n",
    "    listStopwords = set(stopwords.words('indonesian'))\n",
    "    text = [w for w in text if not w in listStopwords]\n",
    "    return text\n",
    "\n",
    "def stemmingText(text): # Reducing a word to its word stem that affixes to suffixes and prefixes or to the roots of words\n",
    "    #factory = StemmerFactory()\n",
    "    from nltk.stem import PorterStemmer\n",
    "    stemmer = PorterStemmer()\n",
    "    text = [stemmer.stem(word) for word in text]\n",
    "    return text\n",
    "\n",
    "def toSentence(list_words): # Convert list of words into sentence\n",
    "    sentence = ' '.join(word for word in list_words)\n",
    "    return sentence\n",
    "\n",
    "alay_dict = pd.read_csv('colloquial-indonesian-lexicon.csv', encoding='latin-1', header=None)\n",
    "alay_dict = alay_dict.rename(columns={0: 'original', 1: 'replacement'})\n",
    "alay_dict_map = dict(zip(alay_dict['original'], alay_dict['replacement']))\n",
    "\n",
    "def normalize_alay(text):\n",
    " text = ' '.join([alay_dict_map[word] if word in alay_dict_map else word for word in text.split(' ')])\n",
    " text = re.sub(' +', ' ', text)\n",
    " return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ch/fxrhbppn0ds9grnhwp1j2qfm0000gn/T/ipykernel_47150/3958851375.py:11: FutureWarning: content is deprecated, use rawContent instead\n",
      "  tweets.append([datetime.now().date(), keyword, tweet.date, tweet.user.username, tweet.content, tweet.hashtags,tweet.mentionedUsers, tweet.likeCount, tweet.retweetCount, tweet.replyCount])\n"
     ]
    }
   ],
   "source": [
    "df, now = scraperKey('kereta cepat','2023-01-13','2023-01-18')\n",
    "topic = 'Test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2023-01-31 13:22:06'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>keyword</th>\n",
       "      <th>date</th>\n",
       "      <th>user</th>\n",
       "      <th>tweet</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>mentions</th>\n",
       "      <th>likeCount</th>\n",
       "      <th>retweetCount</th>\n",
       "      <th>replyCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-01-31</td>\n",
       "      <td>kereta cepat</td>\n",
       "      <td>2023-01-17 23:16:06+00:00</td>\n",
       "      <td>Baekamee</td>\n",
       "      <td>@sunuy_gans @Adabapaknya1 @worksfess Iyaa kare...</td>\n",
       "      <td>None</td>\n",
       "      <td>[https://twitter.com/sunuy_gans, https://twitt...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-01-31</td>\n",
       "      <td>kereta cepat</td>\n",
       "      <td>2023-01-17 22:59:48+00:00</td>\n",
       "      <td>ayuradzan</td>\n",
       "      <td>Early 18 dah dapat lesen kereta dah. kena cepa...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-01-31</td>\n",
       "      <td>kereta cepat</td>\n",
       "      <td>2023-01-17 22:20:03+00:00</td>\n",
       "      <td>mase_aga</td>\n",
       "      <td>@Adabapaknya1 @sunuy_gans @worksfess Kalo kere...</td>\n",
       "      <td>None</td>\n",
       "      <td>[https://twitter.com/Adabapaknya1, https://twi...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-01-31</td>\n",
       "      <td>kereta cepat</td>\n",
       "      <td>2023-01-17 20:23:14+00:00</td>\n",
       "      <td>Mustafic_Khzan</td>\n",
       "      <td>Penerimaan pajak kita memang besar tapi Yang d...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-01-31</td>\n",
       "      <td>kereta cepat</td>\n",
       "      <td>2023-01-17 20:16:39+00:00</td>\n",
       "      <td>dsyauls</td>\n",
       "      <td>@worksfess Harusnya kereta cepat itu Jakarta -...</td>\n",
       "      <td>None</td>\n",
       "      <td>[https://twitter.com/worksfess]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>2023-01-31</td>\n",
       "      <td>kereta cepat</td>\n",
       "      <td>2023-01-13 01:37:36+00:00</td>\n",
       "      <td>handokosupraja</td>\n",
       "      <td>@endonesiatwit @Dennysiregar7 lu tua geblek pu...</td>\n",
       "      <td>None</td>\n",
       "      <td>[https://twitter.com/endonesiatwit, https://tw...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>2023-01-31</td>\n",
       "      <td>kereta cepat</td>\n",
       "      <td>2023-01-13 01:35:11+00:00</td>\n",
       "      <td>mssyahira</td>\n",
       "      <td>Kereta boleh dapat cepat ni kalau Ativa Av, At...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>2023-01-31</td>\n",
       "      <td>kereta cepat</td>\n",
       "      <td>2023-01-13 01:33:53+00:00</td>\n",
       "      <td>mssyahira</td>\n",
       "      <td>Pesanan khidmat masyarakat untuk sesiapa yang ...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>51</td>\n",
       "      <td>28</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>2023-01-31</td>\n",
       "      <td>kereta cepat</td>\n",
       "      <td>2023-01-13 00:27:01+00:00</td>\n",
       "      <td>abdul_karel</td>\n",
       "      <td>@Al4mSyahputra Krisis gara2  pinjam duit tdk p...</td>\n",
       "      <td>None</td>\n",
       "      <td>[https://twitter.com/Al4mSyahputra]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>2023-01-31</td>\n",
       "      <td>kereta cepat</td>\n",
       "      <td>2023-01-13 00:20:27+00:00</td>\n",
       "      <td>giginpraginanto</td>\n",
       "      <td>Para pengendali di pusat kekuasaan kelihatan s...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>213</td>\n",
       "      <td>105</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>504 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      timestamp       keyword                      date             user  \\\n",
       "0    2023-01-31  kereta cepat 2023-01-17 23:16:06+00:00         Baekamee   \n",
       "1    2023-01-31  kereta cepat 2023-01-17 22:59:48+00:00        ayuradzan   \n",
       "2    2023-01-31  kereta cepat 2023-01-17 22:20:03+00:00         mase_aga   \n",
       "3    2023-01-31  kereta cepat 2023-01-17 20:23:14+00:00   Mustafic_Khzan   \n",
       "4    2023-01-31  kereta cepat 2023-01-17 20:16:39+00:00          dsyauls   \n",
       "..          ...           ...                       ...              ...   \n",
       "499  2023-01-31  kereta cepat 2023-01-13 01:37:36+00:00   handokosupraja   \n",
       "500  2023-01-31  kereta cepat 2023-01-13 01:35:11+00:00        mssyahira   \n",
       "501  2023-01-31  kereta cepat 2023-01-13 01:33:53+00:00        mssyahira   \n",
       "502  2023-01-31  kereta cepat 2023-01-13 00:27:01+00:00      abdul_karel   \n",
       "503  2023-01-31  kereta cepat 2023-01-13 00:20:27+00:00  giginpraginanto   \n",
       "\n",
       "                                                 tweet hashtags  \\\n",
       "0    @sunuy_gans @Adabapaknya1 @worksfess Iyaa kare...     None   \n",
       "1    Early 18 dah dapat lesen kereta dah. kena cepa...     None   \n",
       "2    @Adabapaknya1 @sunuy_gans @worksfess Kalo kere...     None   \n",
       "3    Penerimaan pajak kita memang besar tapi Yang d...     None   \n",
       "4    @worksfess Harusnya kereta cepat itu Jakarta -...     None   \n",
       "..                                                 ...      ...   \n",
       "499  @endonesiatwit @Dennysiregar7 lu tua geblek pu...     None   \n",
       "500  Kereta boleh dapat cepat ni kalau Ativa Av, At...     None   \n",
       "501  Pesanan khidmat masyarakat untuk sesiapa yang ...     None   \n",
       "502  @Al4mSyahputra Krisis gara2  pinjam duit tdk p...     None   \n",
       "503  Para pengendali di pusat kekuasaan kelihatan s...     None   \n",
       "\n",
       "                                              mentions  likeCount  \\\n",
       "0    [https://twitter.com/sunuy_gans, https://twitt...          1   \n",
       "1                                                 None          0   \n",
       "2    [https://twitter.com/Adabapaknya1, https://twi...          1   \n",
       "3                                                 None          0   \n",
       "4                      [https://twitter.com/worksfess]          0   \n",
       "..                                                 ...        ...   \n",
       "499  [https://twitter.com/endonesiatwit, https://tw...          0   \n",
       "500                                               None          2   \n",
       "501                                               None         51   \n",
       "502                [https://twitter.com/Al4mSyahputra]          1   \n",
       "503                                               None        213   \n",
       "\n",
       "     retweetCount  replyCount  \n",
       "0               0           0  \n",
       "1               0           1  \n",
       "2               0           1  \n",
       "3               0           0  \n",
       "4               0           0  \n",
       "..            ...         ...  \n",
       "499             0           2  \n",
       "500             0           1  \n",
       "501            28           2  \n",
       "502             0           0  \n",
       "503           105          19  \n",
       "\n",
       "[504 rows x 10 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<built-in function id>\n"
     ]
    }
   ],
   "source": [
    "# id = requests.get('{}/progress/twitter/id/{}'.format(APIurl,now)).json()[0]['id']\n",
    "print(id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(code):\n",
    "    requests.put('{}/progress/twitter/update'.format(APIurl), data={'id':id,'status': code})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sedang Membersihkan Data Twitter...\n",
      "2023-01-31 13:25:40.462646\n",
      "cleaning text\n",
      "2023-01-31 13:25:45.221440\n",
      "casefolding text\n",
      "2023-01-31 13:25:45.221537\n",
      "tokenizing text\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[39mprint\u001b[39m(datetime\u001b[39m.\u001b[39mnow())\n\u001b[1;32m     21\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mtokenizing text\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 22\u001b[0m dataset[\u001b[39m'\u001b[39m\u001b[39mtext_preprocessed\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m dataset[\u001b[39m'\u001b[39;49m\u001b[39mtext_clean\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mapply(tokenizingText)\n\u001b[1;32m     23\u001b[0m \u001b[39mprint\u001b[39m(datetime\u001b[39m.\u001b[39mnow())\n\u001b[1;32m     24\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mfiltering text\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/envs/pySA/lib/python3.9/site-packages/pandas/core/series.py:4771\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4661\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\n\u001b[1;32m   4662\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   4663\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4666\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   4667\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[1;32m   4668\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   4669\u001b[0m \u001b[39m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4670\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4769\u001b[0m \u001b[39m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4770\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4771\u001b[0m     \u001b[39mreturn\u001b[39;00m SeriesApply(\u001b[39mself\u001b[39;49m, func, convert_dtype, args, kwargs)\u001b[39m.\u001b[39;49mapply()\n",
      "File \u001b[0;32m~/miniforge3/envs/pySA/lib/python3.9/site-packages/pandas/core/apply.py:1105\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1102\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_str()\n\u001b[1;32m   1104\u001b[0m \u001b[39m# self.f is Callable\u001b[39;00m\n\u001b[0;32m-> 1105\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[0;32m~/miniforge3/envs/pySA/lib/python3.9/site-packages/pandas/core/apply.py:1156\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1154\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1155\u001b[0m         values \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m)\u001b[39m.\u001b[39m_values\n\u001b[0;32m-> 1156\u001b[0m         mapped \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39;49mmap_infer(\n\u001b[1;32m   1157\u001b[0m             values,\n\u001b[1;32m   1158\u001b[0m             f,\n\u001b[1;32m   1159\u001b[0m             convert\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_dtype,\n\u001b[1;32m   1160\u001b[0m         )\n\u001b[1;32m   1162\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mapped) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(mapped[\u001b[39m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1163\u001b[0m     \u001b[39m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1164\u001b[0m     \u001b[39m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1165\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39m_constructor_expanddim(\u001b[39mlist\u001b[39m(mapped), index\u001b[39m=\u001b[39mobj\u001b[39m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/miniforge3/envs/pySA/lib/python3.9/site-packages/pandas/_libs/lib.pyx:2918\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[15], line 21\u001b[0m, in \u001b[0;36mtokenizingText\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtokenizingText\u001b[39m(text): \u001b[39m# Tokenizing or splitting a string, text into a list of tokens\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m     text \u001b[39m=\u001b[39m word_tokenize(text) \n\u001b[1;32m     22\u001b[0m     \u001b[39mreturn\u001b[39;00m text\n",
      "File \u001b[0;32m~/miniforge3/envs/pySA/lib/python3.9/site-packages/nltk/tokenize/__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mword_tokenize\u001b[39m(text, language\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m, preserve_line\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    115\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[39m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[39m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39m    :type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m     sentences \u001b[39m=\u001b[39m [text] \u001b[39mif\u001b[39;00m preserve_line \u001b[39melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[1;32m    130\u001b[0m     \u001b[39mreturn\u001b[39;00m [\n\u001b[1;32m    131\u001b[0m         token \u001b[39mfor\u001b[39;00m sent \u001b[39min\u001b[39;00m sentences \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m _treebank_word_tokenizer\u001b[39m.\u001b[39mtokenize(sent)\n\u001b[1;32m    132\u001b[0m     ]\n",
      "File \u001b[0;32m~/miniforge3/envs/pySA/lib/python3.9/site-packages/nltk/tokenize/__init__.py:107\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[39mReturn a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[39musing NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[39m:param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    106\u001b[0m tokenizer \u001b[39m=\u001b[39m load(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtokenizers/punkt/\u001b[39m\u001b[39m{\u001b[39;00mlanguage\u001b[39m}\u001b[39;00m\u001b[39m.pickle\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 107\u001b[0m \u001b[39mreturn\u001b[39;00m tokenizer\u001b[39m.\u001b[39;49mtokenize(text)\n",
      "File \u001b[0;32m~/miniforge3/envs/pySA/lib/python3.9/site-packages/nltk/tokenize/punkt.py:1281\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.tokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1277\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtokenize\u001b[39m(\u001b[39mself\u001b[39m, text: \u001b[39mstr\u001b[39m, realign_boundaries: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[\u001b[39mstr\u001b[39m]:\n\u001b[1;32m   1278\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1279\u001b[0m \u001b[39m    Given a text, returns a list of the sentences in that text.\u001b[39;00m\n\u001b[1;32m   1280\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1281\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msentences_from_text(text, realign_boundaries))\n",
      "File \u001b[0;32m~/miniforge3/envs/pySA/lib/python3.9/site-packages/nltk/tokenize/punkt.py:1341\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.sentences_from_text\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1332\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msentences_from_text\u001b[39m(\n\u001b[1;32m   1333\u001b[0m     \u001b[39mself\u001b[39m, text: \u001b[39mstr\u001b[39m, realign_boundaries: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   1334\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[\u001b[39mstr\u001b[39m]:\n\u001b[1;32m   1335\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1336\u001b[0m \u001b[39m    Given a text, generates the sentences in that text by only\u001b[39;00m\n\u001b[1;32m   1337\u001b[0m \u001b[39m    testing candidate sentence breaks. If realign_boundaries is\u001b[39;00m\n\u001b[1;32m   1338\u001b[0m \u001b[39m    True, includes in the sentence closing punctuation that\u001b[39;00m\n\u001b[1;32m   1339\u001b[0m \u001b[39m    follows the period.\u001b[39;00m\n\u001b[1;32m   1340\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1341\u001b[0m     \u001b[39mreturn\u001b[39;00m [text[s:e] \u001b[39mfor\u001b[39;00m s, e \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mspan_tokenize(text, realign_boundaries)]\n",
      "File \u001b[0;32m~/miniforge3/envs/pySA/lib/python3.9/site-packages/nltk/tokenize/punkt.py:1341\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1332\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msentences_from_text\u001b[39m(\n\u001b[1;32m   1333\u001b[0m     \u001b[39mself\u001b[39m, text: \u001b[39mstr\u001b[39m, realign_boundaries: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   1334\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[\u001b[39mstr\u001b[39m]:\n\u001b[1;32m   1335\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1336\u001b[0m \u001b[39m    Given a text, generates the sentences in that text by only\u001b[39;00m\n\u001b[1;32m   1337\u001b[0m \u001b[39m    testing candidate sentence breaks. If realign_boundaries is\u001b[39;00m\n\u001b[1;32m   1338\u001b[0m \u001b[39m    True, includes in the sentence closing punctuation that\u001b[39;00m\n\u001b[1;32m   1339\u001b[0m \u001b[39m    follows the period.\u001b[39;00m\n\u001b[1;32m   1340\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1341\u001b[0m     \u001b[39mreturn\u001b[39;00m [text[s:e] \u001b[39mfor\u001b[39;00m s, e \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mspan_tokenize(text, realign_boundaries)]\n",
      "File \u001b[0;32m~/miniforge3/envs/pySA/lib/python3.9/site-packages/nltk/tokenize/punkt.py:1329\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.span_tokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1327\u001b[0m \u001b[39mif\u001b[39;00m realign_boundaries:\n\u001b[1;32m   1328\u001b[0m     slices \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_realign_boundaries(text, slices)\n\u001b[0;32m-> 1329\u001b[0m \u001b[39mfor\u001b[39;00m sentence \u001b[39min\u001b[39;00m slices:\n\u001b[1;32m   1330\u001b[0m     \u001b[39myield\u001b[39;00m (sentence\u001b[39m.\u001b[39mstart, sentence\u001b[39m.\u001b[39mstop)\n",
      "File \u001b[0;32m~/miniforge3/envs/pySA/lib/python3.9/site-packages/nltk/tokenize/punkt.py:1459\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._realign_boundaries\u001b[0;34m(self, text, slices)\u001b[0m\n\u001b[1;32m   1446\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1447\u001b[0m \u001b[39mAttempts to realign punctuation that falls after the period but\u001b[39;00m\n\u001b[1;32m   1448\u001b[0m \u001b[39mshould otherwise be included in the same sentence.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1456\u001b[0m \u001b[39m    [\"(Sent1.)\", \"Sent2.\"].\u001b[39;00m\n\u001b[1;32m   1457\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1458\u001b[0m realign \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m-> 1459\u001b[0m \u001b[39mfor\u001b[39;00m sentence1, sentence2 \u001b[39min\u001b[39;00m _pair_iter(slices):\n\u001b[1;32m   1460\u001b[0m     sentence1 \u001b[39m=\u001b[39m \u001b[39mslice\u001b[39m(sentence1\u001b[39m.\u001b[39mstart \u001b[39m+\u001b[39m realign, sentence1\u001b[39m.\u001b[39mstop)\n\u001b[1;32m   1461\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m sentence2:\n",
      "File \u001b[0;32m~/miniforge3/envs/pySA/lib/python3.9/site-packages/nltk/tokenize/punkt.py:321\u001b[0m, in \u001b[0;36m_pair_iter\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    319\u001b[0m iterator \u001b[39m=\u001b[39m \u001b[39miter\u001b[39m(iterator)\n\u001b[1;32m    320\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 321\u001b[0m     prev \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(iterator)\n\u001b[1;32m    322\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[1;32m    323\u001b[0m     \u001b[39mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/pySA/lib/python3.9/site-packages/nltk/tokenize/punkt.py:1431\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._slices_from_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1429\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_slices_from_text\u001b[39m(\u001b[39mself\u001b[39m, text: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Iterator[\u001b[39mslice\u001b[39m]:\n\u001b[1;32m   1430\u001b[0m     last_break \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m-> 1431\u001b[0m     \u001b[39mfor\u001b[39;00m match, context \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_match_potential_end_contexts(text):\n\u001b[1;32m   1432\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtext_contains_sentbreak(context):\n\u001b[1;32m   1433\u001b[0m             \u001b[39myield\u001b[39;00m \u001b[39mslice\u001b[39m(last_break, match\u001b[39m.\u001b[39mend())\n",
      "File \u001b[0;32m~/miniforge3/envs/pySA/lib/python3.9/site-packages/nltk/tokenize/punkt.py:1395\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._match_potential_end_contexts\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1393\u001b[0m previous_slice \u001b[39m=\u001b[39m \u001b[39mslice\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m)\n\u001b[1;32m   1394\u001b[0m previous_match \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1395\u001b[0m \u001b[39mfor\u001b[39;00m match \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_lang_vars\u001b[39m.\u001b[39;49mperiod_context_re()\u001b[39m.\u001b[39;49mfinditer(text):\n\u001b[1;32m   1396\u001b[0m \n\u001b[1;32m   1397\u001b[0m     \u001b[39m# Get the slice of the previous word\u001b[39;00m\n\u001b[1;32m   1398\u001b[0m     before_text \u001b[39m=\u001b[39m text[previous_slice\u001b[39m.\u001b[39mstop : match\u001b[39m.\u001b[39mstart()]\n\u001b[1;32m   1399\u001b[0m     index_after_last_space \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_last_whitespace_index(before_text)\n",
      "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "\n",
    "if df.empty:\n",
    "    # update(404)\n",
    "    print(\"Tidak ada data atau query keliru!\")\n",
    "else:\n",
    "    df = df.drop_duplicates(subset=['tweet'])\n",
    "    dataset = df\n",
    "    # update(2)\n",
    "    #print(dataset)\n",
    "    print(\"Sedang Membersihkan Data Twitter...\")\n",
    "    #gabungin hashtags\n",
    "    # dataset['Hashtags'] = '-'.join(dataset['Hashtags'])\n",
    "    print(datetime.now())\n",
    "    print('cleaning text')\n",
    "    dataset['text_clean'] = dataset['tweet'].apply(cleaningText)\n",
    "    print(datetime.now())\n",
    "    print('casefolding text')\n",
    "    # dataset['text_clean'] = dataset['text_clean'].apply(casefoldingText)\n",
    "    # dataset['text_clean'] = dataset['text_clean'].apply(normalize_alay)\n",
    "    \n",
    "    print(datetime.now())\n",
    "    print('tokenizing text')\n",
    "    dataset['text_preprocessed'] = dataset['text_clean'].apply(tokenizingText)\n",
    "    print(datetime.now())\n",
    "    print(\"filtering text\")\n",
    "    # dataset['text_preprocessed'] = dataset['text_preprocessed'].apply(filteringText)\n",
    "    print(datetime.now())\n",
    "    print('stemming text')\n",
    "    # dataset['text_preprocessed'] = dataset['text_preprocessed'].apply(stemmingText)\n",
    "    # dataset[\"popularityScore\"] = (dataset[\"likeCount\"] + dataset[\"retweetCount\"] + dataset[\"replyCount\"])/3\n",
    "    print(\"Selesai Membersihkan Data Twitter :)\")\n",
    "    print(datetime.now())\n",
    "\n",
    "        ###################################\n",
    "        ###      SENTIMENT ANALYSIS     ###\n",
    "        ###################################\n",
    "\n",
    "    print(\"Sedang Menganalisis Sentimen Publik...\")\n",
    "    # update(3)\n",
    "\n",
    "    print(datetime.now())\n",
    "    # Make text preprocessed (tokenized) to untokenized with toSentence Function\n",
    "    X = dataset['text_preprocessed'].apply(toSentence)\n",
    "    X = tfidf_vectorizer.transform(X.values)\n",
    "    \n",
    "    # e = X.toarray()\n",
    "    # #rumus mean\n",
    "    # n = 0\n",
    "    # mean = []\n",
    "    # for i in e:\n",
    "    #     a = sum(i.tolist())/5000\n",
    "    #     mean.append(a)\n",
    "\n",
    "    # #standar deviasi\n",
    "    # n = 0\n",
    "    # stdev = []\n",
    "    # for i in e:\n",
    "    #     s = np.std(i.tolist())\n",
    "    #     stdev.append(s)\n",
    "\n",
    "    # #max value\n",
    "    # n = 0\n",
    "    # maks = []\n",
    "    # for i in e:\n",
    "    #     a = max(i.tolist())\n",
    "    #     maks.append(a)\n",
    "\n",
    "    # #sum\n",
    "    # n = 0\n",
    "    # summ = []\n",
    "    # for i in e:\n",
    "    #     a = sum(i.tolist())\n",
    "    #     summ.append(a)\n",
    "\n",
    "    # #count feature\n",
    "    # n = 0\n",
    "    # count = []\n",
    "    # for i in e:\n",
    "    #     a = sum(map(lambda x : x != 0, i.tolist()))\n",
    "    #     count.append(a)\n",
    "\n",
    "    # extra_X = np.column_stack((e, np.array(mean), np.array(stdev), np.array(maks), np.array(summ), np.array(count)))\n",
    "\n",
    "    # from scipy import sparse\n",
    "    # extra_X = sparse.csr_matrix(extra_X)\n",
    "\n",
    "    y_pred = model.predict(X)\n",
    "    dataset['sentiment'] = y_pred\n",
    "\n",
    "    polarity_decode = {0 : 'Negative', 1 : 'Neutral', 2 : 'Positive'}\n",
    "    dataset['sentiment'] = dataset['sentiment'].map(polarity_decode)\n",
    "    print(\"Selesai Menganalisis Sentimen Publik :)\")\n",
    "\n",
    "    print(datetime.now())\n",
    "\n",
    "    dataset['date'] = pd.to_datetime(dataset['date']).dt.date\n",
    "    dataset['date']=dataset['date'].astype(str)\n",
    "        \n",
    "    print(dataset.dtypes)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 481 entries, 0 to 505\n",
      "Data columns (total 14 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   timestamp          481 non-null    object \n",
      " 1   keyword            481 non-null    object \n",
      " 2   date               481 non-null    object \n",
      " 3   user               481 non-null    object \n",
      " 4   tweet              481 non-null    object \n",
      " 5   hashtags           51 non-null     object \n",
      " 6   mentions           285 non-null    object \n",
      " 7   likeCount          481 non-null    int64  \n",
      " 8   retweetCount       481 non-null    int64  \n",
      " 9   replyCount         481 non-null    int64  \n",
      " 10  text_clean         481 non-null    object \n",
      " 11  text_preprocessed  481 non-null    object \n",
      " 12  popularityScore    481 non-null    float64\n",
      " 13  sentiment          481 non-null    object \n",
      "dtypes: float64(1), int64(3), object(10)\n",
      "memory usage: 56.4+ KB\n"
     ]
    }
   ],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>keyword</th>\n",
       "      <th>date</th>\n",
       "      <th>user</th>\n",
       "      <th>tweet</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>mentions</th>\n",
       "      <th>likeCount</th>\n",
       "      <th>retweetCount</th>\n",
       "      <th>replyCount</th>\n",
       "      <th>popularityScore</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-01-24</td>\n",
       "      <td>kereta cepat</td>\n",
       "      <td>2023-01-17</td>\n",
       "      <td>Baekamee</td>\n",
       "      <td>@sunuy_gans @Adabapaknya1 @worksfess Iyaa kare...</td>\n",
       "      <td>None</td>\n",
       "      <td>[https://twitter.com/sunuy_gans, https://twitt...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-01-24</td>\n",
       "      <td>kereta cepat</td>\n",
       "      <td>2023-01-17</td>\n",
       "      <td>ayuradzan</td>\n",
       "      <td>Early 18 dah dapat lesen kereta dah. kena cepa...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-01-24</td>\n",
       "      <td>kereta cepat</td>\n",
       "      <td>2023-01-17</td>\n",
       "      <td>mase_aga</td>\n",
       "      <td>@Adabapaknya1 @sunuy_gans @worksfess Kalo kere...</td>\n",
       "      <td>None</td>\n",
       "      <td>[https://twitter.com/Adabapaknya1, https://twi...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-01-24</td>\n",
       "      <td>kereta cepat</td>\n",
       "      <td>2023-01-17</td>\n",
       "      <td>Mustafic_Khzan</td>\n",
       "      <td>Penerimaan pajak kita memang besar tapi Yang d...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-01-24</td>\n",
       "      <td>kereta cepat</td>\n",
       "      <td>2023-01-17</td>\n",
       "      <td>dsyauls</td>\n",
       "      <td>@worksfess Harusnya kereta cepat itu Jakarta -...</td>\n",
       "      <td>None</td>\n",
       "      <td>[https://twitter.com/worksfess]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>2023-01-24</td>\n",
       "      <td>kereta cepat</td>\n",
       "      <td>2023-01-13</td>\n",
       "      <td>handokosupraja</td>\n",
       "      <td>@endonesiatwit @Dennysiregar7 lu tua geblek pu...</td>\n",
       "      <td>None</td>\n",
       "      <td>[https://twitter.com/endonesiatwit, https://tw...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>2023-01-24</td>\n",
       "      <td>kereta cepat</td>\n",
       "      <td>2023-01-13</td>\n",
       "      <td>mssyahira</td>\n",
       "      <td>Kereta boleh dapat cepat ni kalau Ativa Av, At...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>2023-01-24</td>\n",
       "      <td>kereta cepat</td>\n",
       "      <td>2023-01-13</td>\n",
       "      <td>mssyahira</td>\n",
       "      <td>Pesanan khidmat masyarakat untuk sesiapa yang ...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>51</td>\n",
       "      <td>28</td>\n",
       "      <td>2</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>2023-01-24</td>\n",
       "      <td>kereta cepat</td>\n",
       "      <td>2023-01-13</td>\n",
       "      <td>abdul_karel</td>\n",
       "      <td>@Al4mSyahputra Krisis gara2  pinjam duit tdk p...</td>\n",
       "      <td>None</td>\n",
       "      <td>[https://twitter.com/Al4mSyahputra]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>2023-01-24</td>\n",
       "      <td>kereta cepat</td>\n",
       "      <td>2023-01-13</td>\n",
       "      <td>giginpraginanto</td>\n",
       "      <td>Para pengendali di pusat kekuasaan kelihatan s...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>216</td>\n",
       "      <td>106</td>\n",
       "      <td>19</td>\n",
       "      <td>113.666667</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>481 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      timestamp       keyword        date             user  \\\n",
       "0    2023-01-24  kereta cepat  2023-01-17         Baekamee   \n",
       "1    2023-01-24  kereta cepat  2023-01-17        ayuradzan   \n",
       "2    2023-01-24  kereta cepat  2023-01-17         mase_aga   \n",
       "3    2023-01-24  kereta cepat  2023-01-17   Mustafic_Khzan   \n",
       "4    2023-01-24  kereta cepat  2023-01-17          dsyauls   \n",
       "..          ...           ...         ...              ...   \n",
       "501  2023-01-24  kereta cepat  2023-01-13   handokosupraja   \n",
       "502  2023-01-24  kereta cepat  2023-01-13        mssyahira   \n",
       "503  2023-01-24  kereta cepat  2023-01-13        mssyahira   \n",
       "504  2023-01-24  kereta cepat  2023-01-13      abdul_karel   \n",
       "505  2023-01-24  kereta cepat  2023-01-13  giginpraginanto   \n",
       "\n",
       "                                                 tweet hashtags  \\\n",
       "0    @sunuy_gans @Adabapaknya1 @worksfess Iyaa kare...     None   \n",
       "1    Early 18 dah dapat lesen kereta dah. kena cepa...     None   \n",
       "2    @Adabapaknya1 @sunuy_gans @worksfess Kalo kere...     None   \n",
       "3    Penerimaan pajak kita memang besar tapi Yang d...     None   \n",
       "4    @worksfess Harusnya kereta cepat itu Jakarta -...     None   \n",
       "..                                                 ...      ...   \n",
       "501  @endonesiatwit @Dennysiregar7 lu tua geblek pu...     None   \n",
       "502  Kereta boleh dapat cepat ni kalau Ativa Av, At...     None   \n",
       "503  Pesanan khidmat masyarakat untuk sesiapa yang ...     None   \n",
       "504  @Al4mSyahputra Krisis gara2  pinjam duit tdk p...     None   \n",
       "505  Para pengendali di pusat kekuasaan kelihatan s...     None   \n",
       "\n",
       "                                              mentions  likeCount  \\\n",
       "0    [https://twitter.com/sunuy_gans, https://twitt...          1   \n",
       "1                                                 None          0   \n",
       "2    [https://twitter.com/Adabapaknya1, https://twi...          1   \n",
       "3                                                 None          0   \n",
       "4                      [https://twitter.com/worksfess]          0   \n",
       "..                                                 ...        ...   \n",
       "501  [https://twitter.com/endonesiatwit, https://tw...          0   \n",
       "502                                               None          2   \n",
       "503                                               None         51   \n",
       "504                [https://twitter.com/Al4mSyahputra]          1   \n",
       "505                                               None        216   \n",
       "\n",
       "     retweetCount  replyCount  popularityScore sentiment  \n",
       "0               0           0         0.333333  Positive  \n",
       "1               0           1         0.333333   Neutral  \n",
       "2               0           1         0.666667   Neutral  \n",
       "3               0           0         0.000000  Negative  \n",
       "4               0           0         0.000000   Neutral  \n",
       "..            ...         ...              ...       ...  \n",
       "501             0           2         0.666667   Neutral  \n",
       "502             0           1         1.000000   Neutral  \n",
       "503            28           2        27.000000   Neutral  \n",
       "504             0           0         0.333333   Neutral  \n",
       "505           106          19       113.666667  Positive  \n",
       "\n",
       "[481 rows x 12 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = dataset[['timestamp', 'keyword', 'date', 'user', 'tweet', 'hashtags', 'mentions', 'likeCount', 'retweetCount', 'replyCount','popularityScore',  'sentiment']]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>keyword</th>\n",
       "      <th>date</th>\n",
       "      <th>user</th>\n",
       "      <th>tweet</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>mentions</th>\n",
       "      <th>likeCount</th>\n",
       "      <th>retweetCount</th>\n",
       "      <th>replyCount</th>\n",
       "      <th>popularityScore</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-01-24</td>\n",
       "      <td>kereta cepat</td>\n",
       "      <td>2023-01-17</td>\n",
       "      <td>Baekamee</td>\n",
       "      <td>@sunuy_gans @Adabapaknya1 @worksfess Iyaa kare...</td>\n",
       "      <td>None</td>\n",
       "      <td>[https://twitter.com/sunuy_gans, https://twitt...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-01-24</td>\n",
       "      <td>kereta cepat</td>\n",
       "      <td>2023-01-17</td>\n",
       "      <td>ayuradzan</td>\n",
       "      <td>Early 18 dah dapat lesen kereta dah. kena cepa...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-01-24</td>\n",
       "      <td>kereta cepat</td>\n",
       "      <td>2023-01-17</td>\n",
       "      <td>mase_aga</td>\n",
       "      <td>@Adabapaknya1 @sunuy_gans @worksfess Kalo kere...</td>\n",
       "      <td>None</td>\n",
       "      <td>[https://twitter.com/Adabapaknya1, https://twi...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-01-24</td>\n",
       "      <td>kereta cepat</td>\n",
       "      <td>2023-01-17</td>\n",
       "      <td>Mustafic_Khzan</td>\n",
       "      <td>Penerimaan pajak kita memang besar tapi Yang d...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-01-24</td>\n",
       "      <td>kereta cepat</td>\n",
       "      <td>2023-01-17</td>\n",
       "      <td>dsyauls</td>\n",
       "      <td>@worksfess Harusnya kereta cepat itu Jakarta -...</td>\n",
       "      <td>None</td>\n",
       "      <td>[https://twitter.com/worksfess]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>2023-01-24</td>\n",
       "      <td>kereta cepat</td>\n",
       "      <td>2023-01-13</td>\n",
       "      <td>handokosupraja</td>\n",
       "      <td>@endonesiatwit @Dennysiregar7 lu tua geblek pu...</td>\n",
       "      <td>None</td>\n",
       "      <td>[https://twitter.com/endonesiatwit, https://tw...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>2023-01-24</td>\n",
       "      <td>kereta cepat</td>\n",
       "      <td>2023-01-13</td>\n",
       "      <td>mssyahira</td>\n",
       "      <td>Kereta boleh dapat cepat ni kalau Ativa Av, At...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>2023-01-24</td>\n",
       "      <td>kereta cepat</td>\n",
       "      <td>2023-01-13</td>\n",
       "      <td>mssyahira</td>\n",
       "      <td>Pesanan khidmat masyarakat untuk sesiapa yang ...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>51</td>\n",
       "      <td>28</td>\n",
       "      <td>2</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>2023-01-24</td>\n",
       "      <td>kereta cepat</td>\n",
       "      <td>2023-01-13</td>\n",
       "      <td>abdul_karel</td>\n",
       "      <td>@Al4mSyahputra Krisis gara2  pinjam duit tdk p...</td>\n",
       "      <td>None</td>\n",
       "      <td>[https://twitter.com/Al4mSyahputra]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>2023-01-24</td>\n",
       "      <td>kereta cepat</td>\n",
       "      <td>2023-01-13</td>\n",
       "      <td>giginpraginanto</td>\n",
       "      <td>Para pengendali di pusat kekuasaan kelihatan s...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>216</td>\n",
       "      <td>106</td>\n",
       "      <td>19</td>\n",
       "      <td>113.666667</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>481 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      timestamp       keyword        date             user  \\\n",
       "0    2023-01-24  kereta cepat  2023-01-17         Baekamee   \n",
       "1    2023-01-24  kereta cepat  2023-01-17        ayuradzan   \n",
       "2    2023-01-24  kereta cepat  2023-01-17         mase_aga   \n",
       "3    2023-01-24  kereta cepat  2023-01-17   Mustafic_Khzan   \n",
       "4    2023-01-24  kereta cepat  2023-01-17          dsyauls   \n",
       "..          ...           ...         ...              ...   \n",
       "501  2023-01-24  kereta cepat  2023-01-13   handokosupraja   \n",
       "502  2023-01-24  kereta cepat  2023-01-13        mssyahira   \n",
       "503  2023-01-24  kereta cepat  2023-01-13        mssyahira   \n",
       "504  2023-01-24  kereta cepat  2023-01-13      abdul_karel   \n",
       "505  2023-01-24  kereta cepat  2023-01-13  giginpraginanto   \n",
       "\n",
       "                                                 tweet hashtags  \\\n",
       "0    @sunuy_gans @Adabapaknya1 @worksfess Iyaa kare...     None   \n",
       "1    Early 18 dah dapat lesen kereta dah. kena cepa...     None   \n",
       "2    @Adabapaknya1 @sunuy_gans @worksfess Kalo kere...     None   \n",
       "3    Penerimaan pajak kita memang besar tapi Yang d...     None   \n",
       "4    @worksfess Harusnya kereta cepat itu Jakarta -...     None   \n",
       "..                                                 ...      ...   \n",
       "501  @endonesiatwit @Dennysiregar7 lu tua geblek pu...     None   \n",
       "502  Kereta boleh dapat cepat ni kalau Ativa Av, At...     None   \n",
       "503  Pesanan khidmat masyarakat untuk sesiapa yang ...     None   \n",
       "504  @Al4mSyahputra Krisis gara2  pinjam duit tdk p...     None   \n",
       "505  Para pengendali di pusat kekuasaan kelihatan s...     None   \n",
       "\n",
       "                                              mentions  likeCount  \\\n",
       "0    [https://twitter.com/sunuy_gans, https://twitt...          1   \n",
       "1                                                 None          0   \n",
       "2    [https://twitter.com/Adabapaknya1, https://twi...          1   \n",
       "3                                                 None          0   \n",
       "4                      [https://twitter.com/worksfess]          0   \n",
       "..                                                 ...        ...   \n",
       "501  [https://twitter.com/endonesiatwit, https://tw...          0   \n",
       "502                                               None          2   \n",
       "503                                               None         51   \n",
       "504                [https://twitter.com/Al4mSyahputra]          1   \n",
       "505                                               None        216   \n",
       "\n",
       "     retweetCount  replyCount  popularityScore sentiment  \n",
       "0               0           0         0.333333  Positive  \n",
       "1               0           1         0.333333   Neutral  \n",
       "2               0           1         0.666667   Neutral  \n",
       "3               0           0         0.000000  Negative  \n",
       "4               0           0         0.000000   Neutral  \n",
       "..            ...         ...              ...       ...  \n",
       "501             0           2         0.666667   Neutral  \n",
       "502             0           1         1.000000   Neutral  \n",
       "503            28           2        27.000000   Neutral  \n",
       "504             0           0         0.333333   Neutral  \n",
       "505           106          19       113.666667  Positive  \n",
       "\n",
       "[481 rows x 12 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[['timestamp','date']] = df[['timestamp','date']].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# api = df.to_json(\"./test_data/json.json\",orient='records')\n",
    "data_dict = df.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [500]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for i in data_dict:\n",
    "    dicts ={\n",
    "        'dateGet':i['timestamp'],\n",
    "        'keyword':i['keyword'],\n",
    "        'contentDate':i['date'],\n",
    "        'username':i['user'],\n",
    "        'tweet':i['tweet'],\n",
    "        'hashtags':i['hashtags'],\n",
    "        'mentions':i['mentions'],\n",
    "        'likeCount':i['likeCount'],\n",
    "        'retweetCount':i['retweetCount'],\n",
    "        'replyCount':i['replyCount'],\n",
    "        'popularityScore':i['popularityScore'],\n",
    "        'sentiment':i['sentiment'],\n",
    "        'topic':topic\n",
    "        }\n",
    "    # print(dicts)\n",
    "    # c = requests.post('{}/data/twitter'.format(APIurl),dicts )\n",
    "    print(c)\n",
    "\n",
    "update(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleanData[['Tweet', 'Result Prediction', 'text_preprocessed']].to_csv('pemilu.csv')\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hashtags = df[['keyword','date','hashtags','sentiment']]\n",
    "df_hashtags = df_hashtags[df_hashtags['hashtags'].notna()].reset_index()\n",
    "df_mentions = df[['keyword','date','mentions','sentiment']]\n",
    "df_mentions = df_mentions[df_mentions['mentions'].notna()].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mentions = df[['keyword','date','mentions','sentiment']]\n",
    "df_mentions = df_mentions[df_mentions['mentions'].notna()].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>keyword</th>\n",
       "      <th>date</th>\n",
       "      <th>mentions</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>kereta cepat</td>\n",
       "      <td>2023-01-17</td>\n",
       "      <td>[https://twitter.com/sunuy_gans, https://twitt...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>kereta cepat</td>\n",
       "      <td>2023-01-17</td>\n",
       "      <td>[https://twitter.com/Adabapaknya1, https://twi...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>kereta cepat</td>\n",
       "      <td>2023-01-17</td>\n",
       "      <td>[https://twitter.com/worksfess]</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>kereta cepat</td>\n",
       "      <td>2023-01-17</td>\n",
       "      <td>[https://twitter.com/kuasabu2, https://twitter...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "      <td>kereta cepat</td>\n",
       "      <td>2023-01-17</td>\n",
       "      <td>[https://twitter.com/worksfess]</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>497</td>\n",
       "      <td>kereta cepat</td>\n",
       "      <td>2023-01-13</td>\n",
       "      <td>[https://twitter.com/novykayra, https://twitte...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>498</td>\n",
       "      <td>kereta cepat</td>\n",
       "      <td>2023-01-13</td>\n",
       "      <td>[https://twitter.com/txtdaribogor]</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>500</td>\n",
       "      <td>kereta cepat</td>\n",
       "      <td>2023-01-13</td>\n",
       "      <td>[https://twitter.com/Paltiwest, https://twitte...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>501</td>\n",
       "      <td>kereta cepat</td>\n",
       "      <td>2023-01-13</td>\n",
       "      <td>[https://twitter.com/endonesiatwit, https://tw...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>504</td>\n",
       "      <td>kereta cepat</td>\n",
       "      <td>2023-01-13</td>\n",
       "      <td>[https://twitter.com/Al4mSyahputra]</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>285 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     index       keyword        date  \\\n",
       "0        0  kereta cepat  2023-01-17   \n",
       "1        2  kereta cepat  2023-01-17   \n",
       "2        4  kereta cepat  2023-01-17   \n",
       "3        8  kereta cepat  2023-01-17   \n",
       "4       12  kereta cepat  2023-01-17   \n",
       "..     ...           ...         ...   \n",
       "280    497  kereta cepat  2023-01-13   \n",
       "281    498  kereta cepat  2023-01-13   \n",
       "282    500  kereta cepat  2023-01-13   \n",
       "283    501  kereta cepat  2023-01-13   \n",
       "284    504  kereta cepat  2023-01-13   \n",
       "\n",
       "                                              mentions sentiment  \n",
       "0    [https://twitter.com/sunuy_gans, https://twitt...  Positive  \n",
       "1    [https://twitter.com/Adabapaknya1, https://twi...   Neutral  \n",
       "2                      [https://twitter.com/worksfess]   Neutral  \n",
       "3    [https://twitter.com/kuasabu2, https://twitter...   Neutral  \n",
       "4                      [https://twitter.com/worksfess]   Neutral  \n",
       "..                                                 ...       ...  \n",
       "280  [https://twitter.com/novykayra, https://twitte...   Neutral  \n",
       "281                 [https://twitter.com/txtdaribogor]   Neutral  \n",
       "282  [https://twitter.com/Paltiwest, https://twitte...   Neutral  \n",
       "283  [https://twitter.com/endonesiatwit, https://tw...   Neutral  \n",
       "284                [https://twitter.com/Al4mSyahputra]   Neutral  \n",
       "\n",
       "[285 rows x 5 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(df_hashtags.shape[0]):\n",
    "    for j in range(len(df_hashtags['hashtags'][i])):\n",
    "        contentDate = df_hashtags['date'][i]\n",
    "        keyword = df_hashtags['keyword'][i]\n",
    "        hashtag = df_hashtags['hashtags'][i][j]\n",
    "        sentiment = df_hashtags['sentiment'][i]\n",
    "        if hashtag != 0:\n",
    "            requests.post('{}/data/twitter/content'.format(APIurl),{\n",
    "                'contentDate' : contentDate,\n",
    "                'topic' : topic,\n",
    "                'keyword' : keyword,\n",
    "                'content' : hashtag,\n",
    "                'type' : 'hashtag',\n",
    "                'sentiment' : sentiment\n",
    "            } )\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@sunuy_gans\n",
      "@Adabapaknya1\n",
      "@worksfess\n",
      "@Adabapaknya1\n",
      "@sunuy_gans\n",
      "@worksfess\n",
      "@worksfess\n",
      "@kuasabu2\n",
      "@riecho_reeboun\n",
      "@Dennysiregar7\n",
      "@worksfess\n",
      "@kelapakopyorr\n",
      "@KAI121\n",
      "@AirinDatangLagi\n",
      "@worksfess\n",
      "@HwrNfl\n",
      "@marwanFC\n",
      "@worksfess\n",
      "@CommuterLine\n",
      "@Yorkeous\n",
      "@hannajoe20\n",
      "@karniilyas\n",
      "@kaesangp\n",
      "@kaesangp\n",
      "@ridwankamil\n",
      "@worksfess\n",
      "@ridwankamil\n",
      "@democrazymedia\n",
      "@sadlybey\n",
      "@daevus_\n",
      "@marwanFC\n",
      "@worksfess\n",
      "@CommuterLine\n",
      "@jalur5_\n",
      "@worksfess\n",
      "@kemenkomarves\n",
      "@upikxc\n",
      "@LuqmanBeeNKRI\n",
      "@lilcigart\n",
      "@sunuy_gans\n",
      "@worksfess\n",
      "@RuangLucuuu\n",
      "@HelmiFelis_\n",
      "@RuangLucuuu\n",
      "@worksfess\n",
      "@dwambrrr\n",
      "@sunuy_gans\n",
      "@worksfess\n",
      "@worksfess\n",
      "@RuangLucuuu\n",
      "@urcottoncandyy\n",
      "@worksfess\n",
      "@tanyakanrl\n",
      "@BuletinTV3\n",
      "@worksfess\n",
      "@andreasstrp\n",
      "@marwanFC\n",
      "@worksfess\n",
      "@CommuterLine\n",
      "@worksfess\n",
      "@2TitikHitam\n",
      "@hannajoe20\n",
      "@karniilyas\n",
      "@sunuy_gans\n",
      "@worksfess\n",
      "@worksfess\n",
      "@fauzan69899430\n",
      "@Dediy_orlando\n",
      "@Dennysiregar7\n",
      "@ganjarpranowo\n",
      "@jokowi\n",
      "@ridwankamil\n",
      "@antoanto_\n",
      "@dododid64453759\n",
      "@BradHarizz\n",
      "@worksfess\n",
      "@KAI121\n",
      "@KemenBUMN\n",
      "@fauzan69899430\n",
      "@Dediy_orlando\n",
      "@Dennysiregar7\n",
      "@ganjarpranowo\n",
      "@jokowi\n",
      "@ridwankamil\n",
      "@Vanadis88624895\n",
      "@SantorinisSun\n",
      "@nusantara_one\n",
      "@KPK_RI\n",
      "@KejaksaanRI\n",
      "@DivHumas_Polri\n",
      "@CCICPolri\n",
      "@mohmahfudmd\n",
      "@Dennysiregar7\n",
      "@NasDem\n",
      "@jokowi\n",
      "@Rhoemeoadhamz\n",
      "@blank0429\n",
      "@worksfess\n",
      "@worksfess\n",
      "@BobLaser10\n",
      "@RamliRizal\n",
      "@RamliRizal\n",
      "@GheaJhanaLie7\n",
      "@Bool_81\n",
      "@Irwan2yah1\n",
      "@CommuterLine\n",
      "@zarnaout45\n",
      "@OposisiCerdas\n",
      "@TeguhTimurCom\n",
      "@petruspasaribuu\n",
      "@jokowi\n",
      "@blessthefaII_\n",
      "@w_i_d_h_i\n",
      "@pengarang_sajak\n",
      "@Jusup_Rianto\n",
      "@kompascom\n",
      "@AqiaqiH\n",
      "@AirinDatangLagi\n",
      "@SBYudhoyono\n",
      "@RKevin_Ramdhani\n",
      "@JulBayur3\n",
      "@endonesiatwit\n",
      "@KINGsm4kers\n",
      "@karniilyas\n",
      "@AqiaqiH\n",
      "@JoharBaru2024\n",
      "@kartarahardja\n",
      "@ZMannaroy\n",
      "@Outstandjing\n",
      "@OposisiCerdas\n",
      "@detikcom\n",
      "@Pencerah___\n",
      "@Dublin2Eric\n",
      "@PEncarah\n",
      "@Krackovwia\n",
      "@sumadiharis\n",
      "@Dennysiregar7\n",
      "@Askrlfess\n",
      "@agung_dk\n",
      "@indra_mbb\n",
      "@AqiaqiH\n",
      "@jokowi\n",
      "@Askrlfess\n",
      "@Askrlfess\n",
      "@CNN\n",
      "@Askrlfess\n",
      "@MakarunaNow\n",
      "@miskinilmu\n",
      "@maspiyu_aja\n",
      "@Outstandjing\n",
      "@bajulcilik\n",
      "@nusantara_one\n",
      "@mairaeeer\n",
      "@blank0429\n",
      "@Outstandjing\n",
      "@cenie_girl\n",
      "@cenie_girl\n",
      "@Kanseulir__\n",
      "@bang_dua\n",
      "@Kopipait__78\n",
      "@MudasirRomini\n",
      "@Dennysiregar7\n",
      "@MardaniAliSera\n",
      "@ridwankamil\n",
      "@jokowi\n",
      "@wewegomb\n",
      "@Karsono12732862\n",
      "@HisyamMochtar\n",
      "@pantograp\n",
      "@sahabat_kereta\n",
      "@MenHub1\n",
      "@BudiKaryaS\n",
      "@KeretaCepatID\n",
      "@kejususu00\n",
      "@Jamaludinsoleh\n",
      "@_NazriH\n",
      "@ismailfahmi\n",
      "@susiair\n",
      "@SimanjuntakElly\n",
      "@Raka_7804\n",
      "@BkBahrul\n",
      "@RelawanNusanta1\n",
      "@JKRSelangor\n",
      "@psi_id\n",
      "@ismailfahmi\n",
      "@susiair\n",
      "@OposisiCerdas\n",
      "@shimee_ya\n",
      "@RKevin_Ramdhani\n",
      "@tokidokicircle\n",
      "@SemutMerah2024\n",
      "@suara1hati\n",
      "@shimee_ya\n",
      "@kartarahardja\n",
      "@BecengAparat\n",
      "@kurawa\n",
      "@jijieazrin\n",
      "@farhan_fevrier\n",
      "@ngabdul\n",
      "@myPKPS\n",
      "@AnkerTwiter\n",
      "@kartarahardja\n",
      "@ZMannaroy\n",
      "@muannas_alaidid\n",
      "@MudasirRomini\n",
      "@Dennysiregar7\n",
      "@MardaniAliSera\n",
      "@ridwankamil\n",
      "@BangEdiii\n",
      "@Kedai_Fakarezo\n",
      "@jawapos\n",
      "@yuwyutik\n",
      "@LittleJ98595985\n",
      "@DokterTifa\n",
      "@DokterTifa\n",
      "@KAI121\n",
      "@cindy_pon\n",
      "@arbellesov\n",
      "@faridgaban\n",
      "@convomf\n",
      "@OposisiCerdas\n",
      "@beetrying\n",
      "@nalar_logis\n",
      "@tatakujiyati\n",
      "@msaid_didu\n",
      "@jokowi\n",
      "@ssefnum\n",
      "@olahantempe\n",
      "@f_gilik\n",
      "@txtdrjkt\n",
      "@CNNIndonesia\n",
      "@aerisugarr\n",
      "@gururidho\n",
      "@RMSigalingging1\n",
      "@aniesbaswedan\n",
      "@cityoflondon\n",
      "@baliandra2\n",
      "@Dennysiregar7\n",
      "@NasDem\n",
      "@Dennysiregar7\n",
      "@NasDem\n",
      "@RXsuci\n",
      "@mynameisowi\n",
      "@Dennysiregar7\n",
      "@NasDem\n",
      "@DokterTifa\n",
      "@Miduk17\n",
      "@ngabdul\n",
      "@BangEdiii\n",
      "@tanyakanrl\n",
      "@ngabdul\n",
      "@kartarahardja\n",
      "@BecengAparat\n",
      "@detikcom\n",
      "@RajaJuliAntoni\n",
      "@temponewsroom\n",
      "@kompascom\n",
      "@detikcom\n",
      "@TirtoID\n",
      "@AimanWitjaksono\n",
      "@KompasTV\n",
      "@tvOneNews\n",
      "@Metro_TV\n",
      "@MaiAbdi1\n",
      "@Miduk17\n",
      "@baliandra2\n",
      "@NagaVVijaya\n",
      "@Dennysiregar7\n",
      "@NasDem\n",
      "@wawanfa17\n",
      "@cR17imo\n",
      "@idextratime\n",
      "@DokterTifa\n",
      "@santri_keliling\n",
      "@SiaranBolaLive\n",
      "@RadenmasNugroho\n",
      "@AbdulKh29602993\n",
      "@Tita83079013\n",
      "@ridwankamil\n",
      "@RoninBasis\n",
      "@Mukidi_alNgibul\n",
      "@jinjelek\n",
      "@baliandra2\n",
      "@BangEdiii\n",
      "@BangEdiii\n",
      "@psi_id\n",
      "@Bernardo_susant\n",
      "@anggahandika20\n",
      "@SiaranBolaLive\n",
      "@TofikHi85890660\n",
      "@aniesbaswedan\n",
      "@cityoflondon\n",
      "@acaiijawe\n",
      "@detikcom\n",
      "@DokterTifa\n",
      "@psi_id\n",
      "@chefbane1\n",
      "@Gherucokro1\n",
      "@BangEdiii\n",
      "@Tita83079013\n",
      "@ch_chotimah2\n",
      "@Pradaprasetya21\n",
      "@TeddGus\n",
      "@SBYudhoyono\n",
      "@ikramwiese\n",
      "@Tita83079013\n",
      "@ridwankamil\n",
      "@gumpnhell\n",
      "@JSuryoP1\n",
      "@_TNIAU\n",
      "@diongideon\n",
      "@AdeDani03\n",
      "@ZeboLady\n",
      "@pooyoyo\n",
      "@_bobobaebae\n",
      "@convomfs\n",
      "@Bernardo_susant\n",
      "@SiaranBolaLive\n",
      "@Azhar_k18\n",
      "@kabarpenumpang\n",
      "@BangEdiii\n",
      "@Paltiwest\n",
      "@bagus_carito\n",
      "@fedriza\n",
      "@SiaranBolaLive\n",
      "@asrori_kudus\n",
      "@Paltiwest\n",
      "@psi_id\n",
      "@SiaranBolaLive\n",
      "@tanyakanrl\n",
      "@jllmisai\n",
      "@Abdillahonim\n",
      "@Askrlfess\n",
      "@BangEdiii\n",
      "@CeritaCintaFess\n",
      "@SiaranBolaLive\n",
      "@BangEdiii\n",
      "@Midjan_La_2\n",
      "@wahju_wibowo\n",
      "@SiaranBolaLive\n",
      "@SiaranBolaLive\n",
      "@BangEdiii\n",
      "@YooStoleMaHeart\n",
      "@SiaranBolaLive\n",
      "@eveantoinette_\n",
      "@KabarGolkarCom\n",
      "@ismailfahmi\n",
      "@susiair\n",
      "@BangEdiii\n",
      "@susipudjiastuti\n",
      "@DPR_RI\n",
      "@NamungAbdiGusti\n",
      "@kelapamuddah\n",
      "@Nagashima007\n",
      "@baliandra2\n",
      "@Bunga_Maw4R6027\n",
      "@Nagashima007\n",
      "@baliandra2\n",
      "@Bunga_Maw4R6027\n",
      "@Twt_Cyberjaya\n",
      "@psi_id\n",
      "@baliandra2\n",
      "@Paltiwest\n",
      "@ikn_id\n",
      "@kurawa\n",
      "@PSI_Jakarta\n",
      "@xnakw2\n",
      "@rezhend\n",
      "@VIVAcoid\n",
      "@baliandra2\n",
      "@rasjawa\n",
      "@rizkidwika\n",
      "@ZeboLady\n",
      "@WinnerWave_\n",
      "@baliandra2\n",
      "@naiys__\n",
      "@RAP1612RAP\n",
      "@susipudjiastuti\n",
      "@Wahyu9\n",
      "@cisirol\n",
      "@raegspatter\n",
      "@baliandra2\n",
      "@JaenoXTM\n",
      "@BangDa079\n",
      "@henrisabastian1\n",
      "@baliandra2\n",
      "@_MbakSri_\n",
      "@kurawa\n",
      "@RoyalFaiiry\n",
      "@Strategi_Bisnis\n",
      "@PriatnaGaluh\n",
      "@KokoGiovanni\n",
      "@KAI121\n",
      "@Pencerah___\n",
      "@Relawananies\n",
      "@PrayogiYulistio\n",
      "@FerryFurqond\n",
      "@VIVAcoid\n",
      "@UniofOxford\n",
      "@hubdat151\n",
      "@convomf\n",
      "@indowfofficial\n",
      "@KPK_RI\n",
      "@Toklexloveunyil\n",
      "@panca66\n",
      "@Askrlfess\n",
      "@Miduk17\n",
      "@SiaranBolaLive\n",
      "@w1nker\n",
      "@TarunaAdjie1\n",
      "@kurawa\n",
      "@BungDenni\n",
      "@yusuf_dumdum\n",
      "@detikcom\n",
      "@tatakujiyati\n",
      "@yusuf_dumdum\n",
      "@detikcom\n",
      "@tatakujiyati\n",
      "@starfess\n",
      "@aqua_gak\n",
      "@ArdjunaWiwaha3l\n",
      "@23wbmy\n",
      "@OposisiCerdas\n",
      "@OposisiCerdas\n",
      "@KokoGiovanni\n",
      "@KAI121\n",
      "@Askrlfess\n",
      "@tanyakanrl\n",
      "@txtdrimedia\n",
      "@OposisiCerdas\n",
      "@BakomstraPD\n",
      "@MIrvanDarwin1\n",
      "@BakomstraPD\n",
      "@ryorossi4689\n",
      "@BakomstraPD\n",
      "@PonorogoSupra\n",
      "@Hasbil_Lbs\n",
      "@wortelsoup\n",
      "@bigwinjanuarii\n",
      "@tanyakanrl\n",
      "@rgantas\n",
      "@ArdjunaWiwaha3l\n",
      "@kurawa\n",
      "@kaypdf\n",
      "@kurawa\n",
      "@kurawa\n",
      "@kurawa\n",
      "@kurawa\n",
      "@AnthonyBudiawan\n",
      "@SahabatSaber\n",
      "@kirim_aja\n",
      "@maspiyu_aja\n",
      "@Fahrihamzah\n",
      "@novykayra\n",
      "@BBCIndonesia\n",
      "@PresidenKopi\n",
      "@jokowi\n",
      "@tisudankapas\n",
      "@Miduk17\n",
      "@suryadijepe\n",
      "@tanyakanrl\n",
      "@yosi_hs\n",
      "@yusuf_dumdum\n",
      "@novykayra\n",
      "@BBCIndonesia\n",
      "@txtdaribogor\n",
      "@Paltiwest\n",
      "@ikn_id\n",
      "@endonesiatwit\n",
      "@Dennysiregar7\n",
      "@Al4mSyahputra\n"
     ]
    }
   ],
   "source": [
    "for i in range(df_mentions.shape[0]):\n",
    "    for j in range(len(df_mentions['mentions'][i])):\n",
    "        contentDate = df_mentions['date'][i]\n",
    "        keyword = df_mentions['keyword'][i]\n",
    "        mention = str(df_mentions['mentions'][i][j]).replace('https://twitter.com/', '@')\n",
    "        sentiment = df_mentions['sentiment'][i]\n",
    "        if mention != 0:\n",
    "            requests.post('{}/data/twitter/content'.format(APIurl),{\n",
    "                'contentDate' : contentDate,\n",
    "                'topic' : topic,\n",
    "                'keyword' : keyword,\n",
    "                'content' : mention,\n",
    "                'type' : 'mention',\n",
    "                'sentiment' : sentiment\n",
    "            } )\n",
    "            print(mention)\n",
    "            \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# SELECT  content,\n",
    "# \tCOUNT( IF( sentiment = 'Positive',1,  NULL) ) AS 'positive',\n",
    "# \tCOUNT( IF( sentiment = 'Neutral',1, NULL) ) AS 'neutral',\n",
    "# \tCOUNT( IF( sentiment = 'Negative',1, Null) ) AS 'negative',\n",
    "# \tCOUNT(sentiment) AS 'total'\n",
    "# FROM twitter_content\n",
    "# WHERE type = 'mention'\n",
    "# GROUP BY content\n",
    "# ORDER BY total desc;"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pySA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "872e9ac453f67b7e35588ebe4d602923bc34b4c5daddde36c325676c5fe52d29"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
